{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:48.489397Z","iopub.execute_input":"2025-08-12T10:28:48.489968Z","iopub.status.idle":"2025-08-12T10:28:48.762027Z","shell.execute_reply.started":"2025-08-12T10:28:48.489943Z","shell.execute_reply":"2025-08-12T10:28:48.761410Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e8/sample_submission.csv\n/kaggle/input/playground-series-s5e8/train.csv\n/kaggle/input/playground-series-s5e8/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# === Libraries ===\nimport os\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:48.763109Z","iopub.execute_input":"2025-08-12T10:28:48.763427Z","iopub.status.idle":"2025-08-12T10:28:53.676776Z","shell.execute_reply.started":"2025-08-12T10:28:48.763409Z","shell.execute_reply":"2025-08-12T10:28:53.676010Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# === Load Data ===\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s5e8/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e8/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/playground-series-s5e8/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:53.677718Z","iopub.execute_input":"2025-08-12T10:28:53.678704Z","iopub.status.idle":"2025-08-12T10:28:56.371391Z","shell.execute_reply.started":"2025-08-12T10:28:53.678675Z","shell.execute_reply":"2025-08-12T10:28:56.370777Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# === Initial Overview ===\ntrain.info()\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:56.372677Z","iopub.execute_input":"2025-08-12T10:28:56.372929Z","iopub.status.idle":"2025-08-12T10:28:56.706109Z","shell.execute_reply.started":"2025-08-12T10:28:56.372911Z","shell.execute_reply":"2025-08-12T10:28:56.705491Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 750000 entries, 0 to 749999\nData columns (total 18 columns):\n #   Column     Non-Null Count   Dtype \n---  ------     --------------   ----- \n 0   id         750000 non-null  int64 \n 1   age        750000 non-null  int64 \n 2   job        750000 non-null  object\n 3   marital    750000 non-null  object\n 4   education  750000 non-null  object\n 5   default    750000 non-null  object\n 6   balance    750000 non-null  int64 \n 7   housing    750000 non-null  object\n 8   loan       750000 non-null  object\n 9   contact    750000 non-null  object\n 10  day        750000 non-null  int64 \n 11  month      750000 non-null  object\n 12  duration   750000 non-null  int64 \n 13  campaign   750000 non-null  int64 \n 14  pdays      750000 non-null  int64 \n 15  previous   750000 non-null  int64 \n 16  poutcome   750000 non-null  object\n 17  y          750000 non-null  int64 \ndtypes: int64(9), object(9)\nmemory usage: 103.0+ MB\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id  age          job  marital  education default  balance housing loan  \\\n0   0   42   technician  married  secondary      no        7      no   no   \n1   1   38  blue-collar  married  secondary      no      514      no   no   \n2   2   36  blue-collar  married  secondary      no      602     yes   no   \n3   3   27      student   single  secondary      no       34     yes   no   \n4   4   26   technician  married  secondary      no      889     yes   no   \n\n    contact  day month  duration  campaign  pdays  previous poutcome  y  \n0  cellular   25   aug       117         3     -1         0  unknown  0  \n1   unknown   18   jun       185         1     -1         0  unknown  0  \n2   unknown   14   may       111         2     -1         0  unknown  0  \n3   unknown   28   may        10         2     -1         0  unknown  0  \n4  cellular    3   feb       902         1     -1         0  unknown  1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>job</th>\n      <th>marital</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>42</td>\n      <td>technician</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>7</td>\n      <td>no</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>25</td>\n      <td>aug</td>\n      <td>117</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>38</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>514</td>\n      <td>no</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>18</td>\n      <td>jun</td>\n      <td>185</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>36</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>602</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>14</td>\n      <td>may</td>\n      <td>111</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>27</td>\n      <td>student</td>\n      <td>single</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>34</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>28</td>\n      <td>may</td>\n      <td>10</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>26</td>\n      <td>technician</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>889</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>3</td>\n      <td>feb</td>\n      <td>902</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# === Check unique values ===\n# Categorical columns we want to inspect\ncategorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n\n# Count unique values for each column and display them\nfor col in categorical_cols:\n    print(f\"\\nColumn: {col}\")\n    print(\"Unique values:\", train[col].unique())\n    print(\"Count of unique values:\", train[col].nunique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:56.706811Z","iopub.execute_input":"2025-08-12T10:28:56.707050Z","iopub.status.idle":"2025-08-12T10:28:57.109622Z","shell.execute_reply.started":"2025-08-12T10:28:56.707024Z","shell.execute_reply":"2025-08-12T10:28:57.108987Z"}},"outputs":[{"name":"stdout","text":"\nColumn: job\nUnique values: ['technician' 'blue-collar' 'student' 'admin.' 'management' 'entrepreneur'\n 'self-employed' 'unknown' 'services' 'retired' 'housemaid' 'unemployed']\nCount of unique values: 12\n\nColumn: marital\nUnique values: ['married' 'single' 'divorced']\nCount of unique values: 3\n\nColumn: education\nUnique values: ['secondary' 'primary' 'tertiary' 'unknown']\nCount of unique values: 4\n\nColumn: contact\nUnique values: ['cellular' 'unknown' 'telephone']\nCount of unique values: 3\n\nColumn: month\nUnique values: ['aug' 'jun' 'may' 'feb' 'apr' 'nov' 'jul' 'jan' 'oct' 'mar' 'sep' 'dec']\nCount of unique values: 12\n\nColumn: poutcome\nUnique values: ['unknown' 'other' 'failure' 'success']\nCount of unique values: 4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# === Visualize Categorical Distributions ===\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set plot style\nsns.set(style=\"whitegrid\")\n\ncategorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n\nplt.figure(figsize=(15, 18))\n\nfor i, col in enumerate(categorical_cols, 1):\n    plt.subplot(3, 2, i)\n    order = train[col].value_counts().index  # sort bars by frequency\n    sns.countplot(data=train, x=col, order=order, palette=\"viridis\")\n    plt.title(f\"{col} distribution\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:28:57.110319Z","iopub.execute_input":"2025-08-12T10:28:57.110504Z","iopub.status.idle":"2025-08-12T10:29:00.516808Z","shell.execute_reply.started":"2025-08-12T10:28:57.110488Z","shell.execute_reply":"2025-08-12T10:29:00.516026Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1294610860.py:18: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n/tmp/ipykernel_36/1294610860.py:18: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n/tmp/ipykernel_36/1294610860.py:18: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n/tmp/ipykernel_36/1294610860.py:18: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n/tmp/ipykernel_36/1294610860.py:18: UserWarning: The figure layout has changed to tight\n  plt.tight_layout()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1800 with 6 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Map boolean-like columns to 0/1\nbool_map = {'no': 0, 'yes': 1}\nfor col in ['default', 'housing', 'loan']:\n    train[col] = train[col].map(bool_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:00.517743Z","iopub.execute_input":"2025-08-12T10:29:00.518173Z","iopub.status.idle":"2025-08-12T10:29:00.631003Z","shell.execute_reply.started":"2025-08-12T10:29:00.518156Z","shell.execute_reply":"2025-08-12T10:29:00.630415Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from scipy.stats import zscore\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ===== Numeric columns to check =====\nnumeric_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n\n# ===== Step 1: Compute Z-scores =====\nz_scores = np.abs(train[numeric_cols].apply(zscore))\n\n# Outlier mask (True where |Z| > 3)\noutlier_mask = z_scores > 3\n\n# Count how many outliers per column\noutlier_counts = outlier_mask.sum()\n\n# ===== Step 2: Visualize Outlier Counts =====\nplt.figure(figsize=(8,5))\nsns.barplot(x=outlier_counts.index, y=outlier_counts.values, palette=\"viridis\")\nplt.title(\"Number of Outliers per Column (Z-score > 3)\")\nplt.ylabel(\"Outlier Count\")\nplt.xlabel(\"Numeric Feature\")\nplt.xticks(rotation=45)\nplt.show()\n\n# ===== Step 3: Boxplots Before Replacement =====\nfor col in numeric_cols:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(x=train[col], color=\"skyblue\")\n    plt.title(f\"{col} - Before Outlier Handling\")\n    plt.show()\n\n# ===== Step 4: Replace Outliers with Median =====\nthreshold = 3\nfor col in numeric_cols:\n    median_val = train[col].median()\n    z_scores_col = np.abs(zscore(train[col]))\n    train.loc[z_scores_col > threshold, col] = median_val\n\n# ===== Step 5: Boxplots After Replacement =====\nfor col in numeric_cols:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(x=train[col], color=\"lightgreen\")\n    plt.title(f\"{col} - After Outlier Handling\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:00.631805Z","iopub.execute_input":"2025-08-12T10:29:00.632054Z","iopub.status.idle":"2025-08-12T10:29:04.114712Z","shell.execute_reply.started":"2025-08-12T10:29:00.632026Z","shell.execute_reply":"2025-08-12T10:29:04.113948Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAeoAAAGSCAYAAAAsHIKWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxw0lEQVR4nO3deVjU9f7//8egYAmBUl52yjSXDwMKiIWWGx0FM/eyQM3MQis7hql1riSzTlmZncwSy1xyOZkaLlnuiS1qLn1NPGluR0VzKY9FyGaA8P794Y85DDMoq7y0++265rqY97zm9X69n/Oeecx7G2yWZVkCAABG8qjuAQAAgJIR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdRXoISEBNnt9uoehludO3fWmDFjqnsY1Wb58uW655571KJFC4WHh1f3cKrVmDFj1LlzZ6dpdrtdCQkJ1TQicxSvw7Jly2S323XixAnHtEGDBmnQoEHVMTwYhqBGme3cuVMJCQlKT0+v7qGUy/bt22W3251ubdq0UUxMjD7//PNy93v48GHFx8erYcOGGj9+vF555ZVKHHXFffXVVxoyZIjuuOMOhYSEqGvXrpo4caJ+//33cvd5+vRpJSQkaN++fZU40spht9tLfA0Kg3H37t2XeVRA2dWs7gHgypOcnKypU6fqvvvuk6+vr9Nja9eulc1mq6aRlc2gQYMUEhIiSUpLS9OaNWv097//XRkZGRo4cGCZ+/vuu+9UUFCgsWPHqlGjRpU93AqZOHGiZs+ercDAQA0dOlR16tTRjz/+qPnz52vVqlWaO3eumjRpUuZ+//vf/2rq1Km6+eabFRQUdMn2P/zwg2rUqFGeRfjT+fDDD6t7CDAEQQ1lZ2erdu3aldKXl5dXpfRzOYSHh+uee+5x3B8wYICioqK0YsWKcgX1b7/9Jkm67rrrKm2M586d07XXXluhPlauXKnZs2ere/fueuuttxxBGR0drb59++rhhx/W008/rU8//VQ1a1btR0KtWrUqra+cnBx5enrKw+Pq3DF4Jb2XULWuzjX8KrJjxw7df//9CgkJUVRUlBYtWuTS5sSJE7Lb7Vq2bJnLY8WPhRUe3z506JCeeeYZtW7dWg8++KAkaf/+/RozZowiIyMVEhKi9u3bKz4+3mnXaEJCgt58801JUmRkpGPXceGxNXfHqI8fP64RI0aoTZs2atmypWJiYvT11187tSncHb169WpNmzZNERERCgkJ0eDBg3Xs2LHyFa+MvLy85Ofn5zasPvvsM/Xt21ehoaFq06aNRo0apZ9//tnxeOfOnR11btu2rUvdP/74Y/Xo0UPBwcHq0KGDXn75ZZdDB4MGDVLPnj21Z88eDRw4UC1bttTbb78tScrNzdWUKVPUpUsXBQcH66677tKbb76p3NzcSy7X1KlT5efnp/Hjx7tszYaGhmro0KE6ePCg1q1b57Q87s41KHrcdPv27XrggQckSfHx8Y51wd16WMjdMerTp08rPj5e7dq1U3BwsHr06KElS5Y4tSlcP1atWqXJkyerY8eOatmypTIzMy+5/KVVmvVf+t976NixYxozZozCw8N1++23Kz4+XufOnXNqm5ubq9dff1133nmnWrVqpWHDhumXX34p1XiKH6Mu63vk448/VmRkpEJDQ/XAAw9ox44dHPe+QrFFbbADBw5oyJAh8vf3V1xcnM6fP6+EhARdf/31Fe776aefVqNGjTRq1CgV/qfTLVu26Pjx4+rbt6/q1aun//znP0pMTNShQ4eUmJgom82mLl266OjRo1q5cqXi4+NVt25dSZK/v7/b+fz666/q37+/zp07p0GDBqlu3br69NNP9eSTTzqCp6iZM2fKZrMpNjZWmZmZmjVrlp599lktXry4wstcXFZWllJTUyVJZ8+e1cqVK3Xw4EG99tprTu2mTZumd999V926ddMDDzyg1NRUzZ8/XwMHDtTy5cvl6+ur559/XsuXL9f69ev1j3/8Q7Vr13ac8JeQkKCpU6eqXbt2GjBggFJSUrRw4ULt3r1bCxculKenp2NeaWlpeuyxx9SjRw/17t1b119/vQoKCvTkk0/q+++/V0xMjJo2baqDBw9q3rx5Onr0qN5///0Sl/Ho0aNKSUlR37595ePj47bNvffeq4SEBH311Vfq0aNHqevXtGlTjRgxQlOmTFG/fv10++23S5Juu+22Uvfx66+/KiYmRjabTQMHDpS/v782btyosWPHKjMzU4888ohT+/fff1+enp4aMmSIcnNznWrnTk5OjuM1Lio7O9tlWmnW/6JGjhypBg0aaPTo0dq7d68WL14sf39//f3vf3e0GTt2rD7//HP17NlTt912m7Zt26bHH3+81PVxpzTvkQULFuiVV15ReHi4HnnkEZ08eVLDhw+Xr6+vbrzxxgrNH5cfQW2wKVOmyLIsffzxx7rpppskSV27dlWvXr0q3HdgYKAmTZrkNO3BBx9UbGys07SwsDCNHj1a33//vcLDwxUYGKjmzZtr5cqVioqKUoMGDS46nxkzZujXX3/Vxx9/7DgLOjo6Wr1799aECRMUGRnptOsyJydHy5cvd+z28/X11WuvvaaDBw8qICCgwstd1PPPP+9038PDQ6NGjXJsJUrSyZMnlZCQoJEjR2rYsGGO6Xfffbfuu+8+LViwQMOGDVNUVJT27dun9evXq2vXro4vLqmpqZo+fbo6dOigmTNnOpa1SZMmeuWVV/T555/r/vvvd/R75swZvfzyy+rfv79j2meffaYtW7boo48+cjqT/P/+7//00ksvaefOnSWG46FDhyTpolcJNGjQQD4+Pjpy5Mgla1bUDTfcoIiICE2ZMkVhYWHq06dPmZ4vSZMnT1Z+fr5WrFjh+NI3YMAAjR49WlOnTlX//v11zTXXONrn5ORo6dKlTtMuZsmSJS5b5yUpzfpfVFBQkF5//XXH/bS0NC1ZssQR1Pv379fnn3+uBx98UC+99JIkaeDAgXrmmWd04MCBUo3JnUu9R3Jzc/Xuu+8qJCRE8+bNc+whstvtGjNmDEF9BWLXt6Hy8/O1efNmRUVFOUJaurAV06FDhwr3XzQIChX/QExNTVXLli0lST/++GO55vPNN98oNDTU6UPO29tb/fr108mTJx1BUqhv375Ox+YKn3f8+PFyzf9ihg8frjlz5mjOnDmaPHmyevToocmTJ2vevHmONuvXr1dBQYG6deum1NRUx+2GG25Qo0aNtH379ovOY8uWLcrLy9PDDz/s9IUkOjpaPj4++uabb5zae3l5qW/fvk7T1q5dq6ZNm6pJkyZOY7jzzjsl6aJjyMrKknSh5hfj7e1dqbuRS8OyLH3xxRfq3LmzLMtyWrYOHTooIyPDZb279957Sx3S0oXDM4WvcdHbkCFDXNqWdf0v/h4KDw9XWlqao46Fr23xXc2DBw8u9fjdudR7ZM+ePUpLS1NMTIzTYZxevXrJz8+vQvNG9WCL2lCpqan6448/3J493LhxY5cP+LJytyWclpamqVOnavXq1Y4TowplZGSUaz6nTp1yfNgVVXiG8alTp5y2lIt+KZHkOKv8YpeC5ebm6uzZs07T/P39L3l2cUBAgNq1a+e43717d2VmZmrSpEnq1auX/P39dfToUVmWpbvvvtttH5c6+erUqVOS5HJGtZeXl2655RadPHnSaXr9+vVdTiI6duyYDh8+rLZt27qdR/HXqqjCgC4M7JJkZWVVyiGVskhNTVV6ero++eQTffLJJyW2KepSe3CKu/HGG51e40LujhOXdf0vaV09e/asfHx8dPLkSXl4eKhhw4ZO7cpzdn1p5lv4Hilc54rPt2bNmrr55psrNG9UD4L6KlDS5VD5+fklPsfd2bcjR45UcnKyhgwZoqCgINWuXVsFBQUaOnSo4zh2VSvpDN6LzT85OVkPP/yw07QNGzaU+UNdku6880599dVX+uGHH/TXv/5VBQUFstlsmjlzptvgr6yz5Qu521osKChQQECA4uPj3T7nYrsymzZtKkkX3dV68uRJZWZmOtpeTH5+fqVdXlVQUCBJ6t27t+677z63bYrvsi/L1nRZlXX9L8+6Whmqa76oPgS1ofz9/XXNNde4PZszJSXF6X7h7qziW52F36xL4+zZs9q6davi4uL01FNPOaYfPXrUpW1ZrpO+6aabXMYryXE8tPjWQXkEBgZqzpw5TtPq1atXrr4Kv9wUnmzUsGFDWZalBg0aqHHjxmXur3D5jhw5oltuucUxPTc3VydOnHC7tVdcw4YNtX//frVt27bM16g3btxYt956qzZs2KDMzEy3J5QtX75cktSpUyfHND8/P7d7MU6dOuW0HBW5Zt7f31/e3t4qKCgoVR2qUlnW/9K6+eabVVBQoJ9++slpK7qs5wKUVeE699NPPzkOj0jS+fPndfLkSWN/1RAl4xi1oWrUqKEOHTooKSnJKXAPHz6szZs3O7X18fFR3bp1tWPHDqfpCxYsKNP83Cl6vLZQ4XW9pdkdftddd+mHH35QcnKyY1p2drYSExN18803q1mzZqUeY0n8/PzUrl07p1t5r9ctvGys8MPs7rvvVo0aNTR16lSXLRbLsi75q17t2rWTp6enPvroI6fnL1myRBkZGbrrrrsuOaZu3brp9OnTSkxMdHnsjz/+cHsGc1HDhw/X2bNn9dJLL7nsZdmzZ49mzZqlgIAAp937t9xyi/797387Xf711VdfOV2SJv1vXSjPr9TVqFFDXbt21bp163Tw4EGXx92drV1VyrL+l1ZERIQk6aOPPqq0PksjODhYderUUWJios6fP++YvmLFCpdDRLgysEVtsLi4OG3atEkDBw7UgAEDlJ+fr/nz56tZs2YuuzKjo6M1Y8YMjR07VsHBwdqxY4fbLdmS+Pj4qHXr1po1a5by8vJUv359ffvtt06/PVyoRYsWki6csdu9e3d5enqqU6dObncDP/7441q1apUee+wxDRo0SH5+flq+fLlOnDihhISEav2xih07dignJ0fShS2qL7/8Ut9995169Ojh2A3csGFDjRw5UpMmTdLJkycVFRUlb29vnThxQklJSYqJiXF7YlIhf39/PfHEE5o6daqGDh2qzp07KyUlRQsWLFBISIh69+59yXH26dNHa9as0UsvvaTt27frtttuU35+vo4cOaK1a9dq1qxZjl9Yc6d3797avXu3/vWvf+nw4cPq1auXfH19tXfvXi1dulR16tTRu+++63SpU3R0tNatW6ehQ4eqW7du+umnn7RixQqX454NGzaUr6+vFi1aJG9vb9WuXVuhoaFOW90X88wzz2j79u2KiYlRdHS0mjVrprNnz+rHH3/U1q1b9d1335Wqn4oqy/pfWkFBQerZs6cWLFigjIwMtWrVStu2bavy3wXw8vJSXFycxo8fr8GDB6tbt246efKkli1b5vL64cpAUBssMDBQH374oSZMmKApU6boxhtvVFxcnM6cOeMS1MOHD1dqaqrWrVunNWvWKCIiQrNmzSrxBCR3Jk2apPHjx2vBggWyLEvt27fXzJkz1bFjR6d2oaGhevrpp7Vo0SJt2rRJBQUF2rBhg9ugvuGGG7Ro0SL985//1Pz585WTkyO73a4PPvhAf/3rX8tVl8pSdEvH09NTt9xyi0aNGuUSvI8//rhuvfVWzZ07V++9956kC8eF27dv7/JPJ9yJi4uTv7+/5s+frwkTJsjPz08xMTEaPXr0Ja8Dli4ck3zvvfc0d+5cffbZZ1q/fr2uvfZaNWjQQIMGDSrVLvmxY8fqjjvu0IIFCzR9+nSdO3dOf/nLXzRw4EA99thjLtfBd+zYUWPGjNGcOXP0+uuvKzg4WB988IEmTpzo1M7T01NvvPGG3n77bf3jH//Q+fPnNWHChFIH9Q033KDFixfrvffe0/r167Vw4ULVqVNHzZo107PPPluqPipLadf/snj99ddVt25drVixQhs2bNAdd9yhGTNmlGpPSkU89NBDsixLc+bM0cSJExUYGKhp06bp1VdfrdRfh8PlYbM4AwEArnoFBQVq27atunTpoldffbW6h4My4Bg1AFxlcnJyXM6pWL58udLS0tSmTZtqGhXKi13fAHCV2bVrlyZMmKB77rlHderU0d69e7VkyRIFBAQ4/SMaXBkIagC4ytx888268cYb9dFHH+ns2bPy8/NTnz599Oyzz/Jfua5AHKMGAMBgHKMGAMBgBDUAAAYr1THq5ORkWZZVqms+AQDA/+Tl5clms6lVq1blen6ptqgty6r0H3y3LEu5ubn8kHwlo65Vg7pWDepaNahr1ShvXSuaoaXaoi7ckr7YzxSWVXZ2tvbt26dmzZpV+n8g+jOjrlWDulYN6lo1qGvVKG9dd+/eXaH5cowaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGA1q3sA7liWpdzcXKf7kmSz2Sp1Pl5eXpXeJwAAlcnIoM7NzdVzzz1X5fOZOHGiatWqVeXzAQCgvNj1DQCAwYzcoi6qUe9HdOzzuZKkxn0fk0dNzwr1V3A+TynLZlbCyAAAqHrGB3XRYPao6VnhoAYA4ErCrm8AAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMVm1BbVmWLMuqrtkbgzoAAC6mZnXM1LIsTZ8+XefOnVNQUFB1DMEIlmVpypQpkqQRI0bIZrNV84gAAKaplqDOzc3VTz/9JEnKy8urjiEYITc3VykpKY6/a9WqVc0jAgCYhmPUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGCwmtU9AFzw3HPPVfcQ/tTCwsK0a9euMj3HZrPJsiynad7e3nrttddc2s6dO1e7du1SWFiY6tWrp6SkJNWsWVO1atVS//79FRwc7NSuTp06SktLc7Rfv369JKlGjRp69NFHdezYMSUlJalWrVr6448/XObn6ekpLy8vDRgwwNF3cXv27NHSpUvVqFEjx7I3btxYkpSSkqLGjRvr6aefdnrO2LFjlZWVVeJylteCBQu0Z88ehYWF6ZFHHqlwf4XLdv/995e4/FeiwvXD29tbWVlZ8vDwUGxsrIKDg7Vnzx4tWrRIlmWpb9++bp9f3rqsWrVKGzZsUGRkpHr06HHRtqbVfs+ePVq4cKFsNpuaNWvmtK6fOXNGlmU53ierVq1yvNfq16+v+Pj4ahz5/7BFXY0IZ3OUNaQluYS0JGVlZengwYNO01JTUx3979q1S+vXr5dlWcrLy1NmZqYSExOVm5vr1C4tLc2pfaH8/HwtXLjQ0Ye7kJakvLw8ZWVlOfouLjc3V4sXL9bvv//utOwpKSlKSUlx/H369GnHYwcPHlRWVlaJy1lemZmZ2rNnj6QLy5uamlqh/oou2+LFi90u/5Wo6PpR+DoUFBRo4cKFjvUoMzNTWVlZWr58uc6fP+/0/PLWJTMzU0lJSSooKFBSUpIyMzNLbGta7XNzc5WYmKisrCxlZma6rOuF9UpMTFRqaqrTe+306dM6ceJENYzaFUENVLL333/f6f6UKVMu2j49PV1JSUmXbFeo8EO6NAr7Li4pKUnp6emXfP7bb7/t+Lv4chW/X17r1q1zul/aOpSk6LKVtPxXopLqkpWVpdmzZzu9nhkZGY4vP4XKW5fZs2c7vpRalqXZs2eX2Na02pd2PU9PT3db38mTJ1fFsMqs2oM6NzdXOTk5TrfL9S3M3bwv123kyJGXZRlRPebOnStJ+u677xxbxxfzxRdflKpdeSQlJenMmTOO+2fOnFFSUpLbPQLF5eTkaMOGDY7lKa6k6aW1c+dOZWdnO01LS0vTd999V67+ii+bZVnasGGD0/JfiS61Hh05csRl2p49e/Tbb79JKn9dDhw44NL3kSNHdODAAZe2ptX+zJkzTlvIl+Kuvvn5+fr8888rcVTlUy3HqIt+QLz++uulblvZ8x43blyl9g0U2rVrl7Kzs7Vo0aLqHooKCgq0ZMkSDRs2TJK0dOlSFRQUlPr5K1asKPGxXbt2KScnR7Vq1SrzuPLz87Vs2TK3jy1atEi33367atSoUer+LMvS0qVLS5z+xBNPyGazlXmc1S0/P79c65FlWfrss8/0t7/9rVx1KSgo0Lx589z2PW/ePL366qvy8PBw6qus86gqlmVpyZIllZIfX375pbp16yZPT89KGFn5VPsWNXC1mjx5cpkCsSodOHBAp0+f1unTp7V///5K7ftiu0IvZsuWLSXWp6CgQFu2bClTf4XLVrzPgoIC7d+/3+l4+5XkYnW6lEOHDmnv3r3lqsvevXtd9nYUys7O1t69ex33Tav96dOn3W71l1dJXygvl2rZoi76zer555+Xn5+f0+O5ubmOrd3K/hZWtL/x48fLy8urUvsvDU4i+3MYNWqUXnjhBSPC2m63q379+pKkwMDASg3r2NjYcj2vXbt2+vTTT93Wx8PDQ+3atStTf/Xr11dgYKAOHjzo1KeHh4cCAgIcy3+luVidLqVZs2Zq3rx5uerSvHlz1a5d221Ye3t7q3nz5o77ptW+fv36stvtlRbWJZ1Ff7lU+xa1l5eXatWq5XS7XOHpbt6X4/bOO+9cluVD9WnVqpVq166t/v37V/dQ5OHhoejoaNlsNtlsNt1///2OXZal0bt3b4WFhbl9rFWrVuXa7S1duNSspA/AAQMGlGm3tyTHsrmb/sADD1yRu72lC3Uqz3pks9nUp08feXh4lKsuHh4eGjx4sNvHBg8e7LQOmVb7ypxvZGRkte72lgwI6j8rwvrqVvgB16ZNG9WpU+eS7e++++5StSuPqKgo3XDDDY779erVU1RUVKk+xGrVqqXOnTuXeG1zSR/kpXXbbbepdu3aTtPq1Kmj1q1bl6u/4stms9kUGRnptPxXokutR02aNHGZFhwcrOuvv15S+etit9td+m7SpIkCAgJc2ppW+3r16qlLly6lbu+uvjVq1FCvXr0qcVTlQ1ADlexvf/ub0/0RI0ZctL2fn5+ioqIu2a6Qt7d3qcdS2HdxUVFR8vX1veTzR48e7fi7+HIVv19eXbt2dbpf2jqUpOiylbT8V6KS6uLj46PY2Fin1/O6665z+bGR8tYlNjbWKXwvdqjDtNqXdj338/NzW99Ro0ZVxbDKjKCuRhMnTqzuIeD/V9Ku3Ytxt0Xq7e3tsrXh7+/v6D8sLExdunSRzWaTp6enfHx8FB0dLS8vL6d2hd/uC9sXqlGjhgYMGODo45prrnE7Nk9PT3l7ezv6Ls7Ly0vR0dGqW7eu07I3btzY8etkjRs3djq2GBAQ4PiS4G45y8vHx8cRKmFhYfL3969Qf0WX7YEHHqiW81CqQtH1o/B18PDwUP/+/eXj46OYmBj5+PjI29tb9957r2rWdD4Fqbx18fHxUVRUlDw8PBQVFSUfH58S25pWey8vL8XExMjb21s+Pj4u63phvaKjo+Xv7+/0Xqtfv74aNGhQDaN2ZbNKcf767t27JUkhISGVMtOcnBzHCVX/+Mc/XHY5FH28cd/HlLJspiSpaczf5FGzYscKCs7n6XDihR9qmDhxYrmPr1WGostZWWPJzs7Wvn37FBQU5LJLEeVHXasGda0a1LVqlLeuFc1QtqgBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGA1q2OmXl5eatSokbKzs+Xp6VkdQzCCl5eXGjdu7PgbAIDiqiWobTabHn/8ce3bt082m606hmAEm82mESNGOP4GAKC4aglq6UIwEU4ENADg4jhGDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADBYzeoewKUUnM9z+3dl9AcAgOmMD+pjn891/J2ybGb1DQQAgGrArm8AAAxm5Ba1l5eXJk6c6LhvWZYkyWazVfp8AAAwmZFBbbPZVKtWreoeBgAA1Y5d3wAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGs1mWZV2q0c6dO2VZlry8vCptxpZlKS8vT56enrLZbJXW758dda0a1LVqUNeqQV2rRnnrmpubK5vNpttuu61c861ZmkZV8ULbbLZKDX5cQF2rBnWtGtS1alDXqlHeutpstgrlaKm2qAEAQPXgGDUAAAYjqAEAMBhBDQCAwQhqAAAMRlADAGAwghoAAIMR1AAAGIygBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADHbZg/rw4cN69NFHFRYWpvbt2+vNN99Ubm7u5R6GkY4dO6YXX3xRffr0UfPmzdWzZ0+37RYvXqyuXbsqJCREvXv31ldffeXSJiMjQ88//7zatGmjVq1aacSIEfrvf//r0m7nzp3q16+fQkND1alTJ82YMUNX2/9pWbNmjZ588klFREQoLCxMffr00ZIlS1yWk7qWzTfffKOHHnpId955p4KDgxUZGakJEyYoIyPDqd2XX36p3r17KyQkRF27dtXSpUtd+srNzdXEiRPVvn17hYWF6dFHH9WRI0dc2v3ZPj+ysrIUEREhu92u3bt3Oz3G+lo2y5Ytk91ud7m99dZbTu2MrKt1GaWlpVnt27e3Bg4caG3cuNFavHixdfvtt1svv/zy5RyGsdavX29FRERYcXFxVs+ePa0ePXq4tFm5cqVlt9utyZMnW1u3brXGjRtnNW/e3EpOTnZqFxsba0VERFirVq2ykpKSrJ49e1q9e/e28vLyHG2OHj1qhYWFWcOHD7e2bNlizZkzx2rRooU1a9asql7UyyomJsYaNWqUtWrVKmvLli3WW2+9ZQUGBloJCQmONtS17JYvX25NnDjRWrt2rbVt2zbro48+stq0aWM9+uijjjb/7//9PysoKMgaN26ctXXrVmvy5MmW3W631qxZ49TXuHHjrNtvv91avHixtXHjRuvBBx+0OnbsaKWnpzva/Bk/P958802rXbt2VkBAgPXDDz84prO+lt3SpUutgIAAa+PGjVZycrLjdurUKUcbU+t6WYP6gw8+sMLCwqzff//dMW3RokVWUFCQ9csvv1zOoRgpPz/f8fdzzz3nNqjvvvtua/To0U7T+vXrZw0dOtRxf+fOnVZAQIC1adMmx7TDhw9bdrvdWrVqlWPauHHjrE6dOlk5OTmOaZMmTbLCw8Odpl3pfvvtN5dpL7zwgnXbbbc5ak5dK8cnn3xiBQQEON7PsbGxVr9+/ZzajB492urWrZvj/s8//2wFBQVZixYtckz7/fffrbCwMGvGjBmOaX+2z49Dhw5ZYWFh1sKFC12CmvW17AqD2t3nQSFT63pZd31v3LhRbdu2VZ06dRzTunXrpoKCAn377beXcyhG8vC4+Mtx/PhxHT16VN26dXOa3r17d23dutWxC3Djxo3y9fVV+/btHW2aNGmioKAgbdy40TFt48aNioyMdPpH6N27d1d6erqSk5MrY5GM4O/v7zItKChImZmZys7Opq6VqPC9nZeXp9zcXG3fvl333HOPU5vu3bvr8OHDOnHihCRp8+bNKigocGpXp04dtW/f3qWuf6bPj1dffVX9+/dX48aNnaazvlYNk+t6WYP6yJEjatKkidM0X19f1atXz+3xKDgrrFHxN27Tpk2Vl5en48ePO9o1btxYNpvNqV2TJk0cfWRnZ+vnn392eT2aNGkim8121b8e33//verXry8fHx/qWkH5+fnKycnRjz/+qPfee0+dO3dWgwYN9NNPPykvL8+lFk2bNpX0v/X5yJEjuv766+Xn5+fSrmi9/kyfH2vXrtXBgwc1fPhwl8dYXyumZ8+eCgoKUmRkpKZPn678/HxJZte1ZplaV1B6erp8fX1dpvv5+ens2bOXcyhXpMIaFa9h4f3Cx9PT03Xddde5PN/Pz0979uyRJMcJP8X78vLy0rXXXntVvx47duzQ6tWr9dxzz0mirhXVqVMnnT59WpLUsWNHTZo0SVLF6+rr6+tUrz/L58e5c+f0xhtvaNSoUfLx8XF5nPW1fOrVq6e4uDi1bNlSNptNX375pd555x2dPn1aL774otF1vaxBDVS3X375RaNGjdIdd9yhhx9+uLqHc1WYMWOGzp07p0OHDmnatGkaNmyY5syZU93DumJNmzZN119/ve6///7qHspVpWPHjurYsaPjfocOHVSrVi3NmzdPw4YNq8aRXdpl3fXt6+vrcumGdOGbSvHdXnBVWKPiNUxPT3d63NfXV5mZmS7PL1rnwm+ExfvKzc3VuXPnrsrXIz09XY899pjq1KmjhIQExzkB1LViAgMD1apVK0VHR+v999/X9u3btX79+grXNT093alef4bPj5MnT2r27NkaMWKEMjIylJ6eruzsbEkXdqdmZWWxvlaibt26KT8/X/v27TO6rpc1qIvuwy+UkZGhM2fOuOzLh6vCGhWv4ZEjR+Tp6albbrnF0S4lJcXler2UlBRHH7Vr19Zf/vIXl74Kn3e1vR5//PGHnnjiCWVkZGjWrFlOu66oa+Wx2+3y9PTUTz/9pIYNG8rT09NtXaX/1b1Jkyb69ddfXXYHFj8m/Wf4/Dhx4oTy8vL0+OOPq3Xr1mrdurVja+/hhx/Wo48+yvpaRUyu62UN6oiICG3ZssXxDUW6cNKEh4eH0xl0cO+WW27RrbfeqrVr1zpNX716tdq2bes4uzAiIkJnz57V1q1bHW1SUlK0d+9eRUREOKZFRERow4YNysvLc+rL19dXrVq1quKluXzOnz+vkSNH6siRI5o1a5bq16/v9Dh1rTz//ve/lZeXpwYNGsjLy0t33HGH1q1b59Rm9erVatq0qRo0aCDpwi5IDw8PffHFF442Z8+e1ebNm13qerV/fgQFBelf//qX0y0+Pl6S9PLLL+ull15ifa1Eq1evVo0aNdS8eXOz61qmi7kqqPAHCx566CFr06ZN1pIlS6zw8PCr+gcLyiI7O9tas2aNtWbNGuuhhx6y7rrrLsf9wmv/VqxYYdntduvdd9+1tm3bZr344otW8+bNrZ07dzr1FRsba911113W6tWrrQ0bNlz0gvy4uDhry5Yt1ty5c6/KHzp44YUXrICAAGv27NlOP3SQnJzsuJ6Rupbd8OHDrWnTpllffvmltWXLFmv27NlW+/btrV69ejnqWviDJy+99JK1bds2691337Xsdru1evVqp77GjRtnhYeHW0uWLLE2bdpkPfTQQyX+4Mmf7fNj27ZtLtdRs76WXWxsrDV9+nTr66+/tr7++mtr3Lhxlt1ut1577TVHG1PrelmD2rIuXMQ/ePBgKzQ01Grbtq31xhtvXFUX1VfE8ePHrYCAALe3bdu2OdolJiZaXbp0sVq0aGH17NnT+vLLL136Sk9Pt+Lj463w8HArLCzMeuqpp9z+KMT3339vRUdHW8HBwVZERIQ1ffp0q6CgoEqX83Lr1KlTiXU9fvy4ox11LZvp06dbffr0sVq1amWFhYVZPXr0sN555x0rIyPDqV3hLze1aNHC6tKli7V48WKXvnJycqw33njDatu2rRUaGmo98sgj1qFDh1za/Rk/P9wFtWWxvpbV+PHjrbvvvtsKDQ21goODrZ49e1rz5s1zWU4T62qzrKvsB10BALiK8N+zAAAwGEENAIDBCGoAAAxGUAMAYDCCGgAAgxHUAAAYjKAGAMBgBDVQRRISEmS326t7GC46d+6sMWPGVPcwAJQSQQ1chXbu3KmEhASn38UGcGXi/1EDV6Hk5GRNnTpV9913n8s/r1+7dq1sNls1jQxAWbFFDVwBCv8ncWXw8vKSp6dnpfUHoGoR1EAl2LFjh+6//36FhIQoKipKixYtcnr8xIkTstvtWrZsmctz7Xa7EhISHPcLj20fOnRIzzzzjFq3bq0HH3xQkrR//36NGTNGkZGRCgkJUfv27RUfH6/ff//d6flvvvmmJCkyMlJ2u112u10nTpyQ5P4Y9fHjxzVixAi1adNGLVu2VExMjL7++munNtu3b5fdbtfq1as1bdo0RUREKCQkRIMHD9axY8fKXzwAF8Wub6CCDhw4oCFDhsjf319xcXE6f/68EhISdP3111eo36efflqNGjXSqFGjHP+kfsuWLTp+/Lj69u2revXq6T//+Y8SExN16NAhJSYmymazqUuXLjp69KhWrlyp+Ph41a1bV5Lk7+/vdj6//vqr+vfvr3PnzmnQoEGqW7euPv30Uz355JOaMmWKunTp4tR+5syZstlsio2NVWZmpmbNmqVnn31WixcvrtDyAnCPoAYqaMqUKbIsSx9//LFuuukmSVLXrl3Vq1evCvUbGBioSZMmOU178MEHFRsb6zQtLCxMo0eP1vfff6/w8HAFBgaqefPmWrlypaKiotSgQYOLzmfGjBn69ddf9fHHHys8PFySFB0drd69e2vChAmKjIyUh8f/dr7l5ORo+fLl8vLykiT5+vrqtdde08GDBxUQEFChZQbgil3fQAXk5+dr8+bNioqKcoS0JDVt2lQdOnSoUN/9+/d3mXbNNdc4/s7JyVFqaqpatmwpSfrxxx/LNZ9vvvlGoaGhjpCWJG9vb/Xr108nT57UoUOHnNr37dvXEdKSHM87fvx4ueYP4OLYogYqIDU1VX/88YcaNWrk8ljjxo31zTfflLtvd1vCaWlpmjp1qlavXq3ffvvN6bGMjIxyzefUqVOOsC+qSZMmjseLbikX/UIiyXFWOZeCAVWDoAYug5Iuh8rPzy/xObVq1XKZNnLkSCUnJ2vIkCEKCgpS7dq1VVBQoKFDhzqOY1e1orvBi7pc8wf+bAhqoAL8/f11zTXXuD3rOSUlxfG3n5+fJNetzlOnTpV6XmfPntXWrVsVFxenp556yjH96NGjLm3Lcp30TTfd5DTWQkeOHHE8DqD6cIwaqIAaNWqoQ4cOSkpKcgrdw4cPa/PmzY77Pj4+qlu3rnbs2OH0/AULFpRpXu7MmzfPZdq1114rqXS7w++66y798MMPSk5OdkzLzs5WYmKibr75ZjVr1qzUYwRQ+diiBiooLi5OmzZt0sCBAzVgwADl5+dr/vz5atasmQ4cOOBoFx0drRkzZmjs2LEKDg7Wjh073G7JlsTHx0etW7fWrFmzlJeXp/r16+vbb791XB9dVIsWLSRJkydPVvfu3eXp6alOnTqpdu3aLm0ff/xxrVq1So899pgGDRokPz8/LV++XCdOnFBCQkKJu7oBXB68A4EKCgwM1Icffqi6detqypQpWrp0qeLi4lyuPx4+fLgeeOABrVu3Tv/85z+Vn5+vWbNmlWlekyZNUocOHbRgwQK9/fbbqlmzpmbOnOnSLjQ0VE8//bT279+v+Ph4jR49WqmpqW77vOGGG7Ro0SK1a9dO8+fP19tvvy1PT0998MEHLssA4PKzWZwBAgCAsdiiBgDAYAQ1AAAGI6gBADAYQQ0AgMEIagAADEZQAwBgMIIaAACDEdQAABiMoAYAwGAENQAABiOoAQAwGEENAIDBCGoAAAz2/wG87uJ2vjYE8wAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAeQAAAGSCAYAAAAy1bIlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx9klEQVR4nO3deVxV1f7/8fdBQVMDonxgOeR0D6igYCqZQymWklM5NSjaVTK7DqVfS9GbWZpmNyunzPRqpunNgSySsjRv995KGr6a5pDlLBnfygFwAOTs3x/9OJftOSASeJbyej4ePHSttc/en7M4m/fZwwGHZVmWAACAT/n5ugAAAEAgAwBgBAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIKPMpaamKiwsTKmpqb4uxWfWrVunLl26qEmTJmrRooWvy/Gp8ePHq2PHjra+sLAwzZkzx0cVmePCeUhKSlJYWJiOHj3q7ouPj1d8fLwvykMZI5CB/y//jUPBr1atWqlfv3567733Srzeffv2KTExUXXq1NGUKVP07LPPlmLVf9zmzZs1ZMgQxcTEKDIyUp07d9aMGTN04sSJEq8zPT1dc+bM0e7du0ux0tIRFhZW6PcgPwB37NhxmasCpIq+LgBXv5YtW2r79u3y9/f3dSnFEh8fr8jISEnSyZMn9cEHH+iJJ55QZmam+vfvf8nr+/LLL+VyuTRx4kTdfPPNpV3uHzJjxgwtXrxY4eHhSkhIUHBwsHbu3Knly5dr/fr1euONN1S/fv1LXu///d//ae7cuapZs6YaNWp00eW3b9+uChUqlOQplDt///vffV0CygiBjDLn5+enSpUq+bqMYmvRooW6dOnibj/wwAPq1KmTkpOTSxTIv/32myTp2muvLbUaz549q2uuueYPreP999/X4sWLdffdd+vFF190B2Lfvn3Vq1cvDRw4UI899pjeeecdVaxYtj8qSvP1kZ2dLX9/f/n5XZ0nAAMCAnxdAsrI1fmKvQKlp6drwoQJatu2rSIiItSxY0c9/fTTysnJkfT7kdqMGTPUvXt3RUdHq3nz5kpISNCePXts68k/7ZqSkqK5c+eqXbt2io6O1qhRo5SZmamcnBw999xzat26taKjo5WYmOjeRr78U3rvvfeeOnfurMjISPXq1UtfffWVbbm0tDRNnjxZnTt3VtOmTRUTE6NRo0bZrncVrOnCa8hvvfWWYmNj1bRpU/Xp00dff/21x/Wxgs9n/vz5at++vSIjIzVo0CAdOnToD897cQQEBCgoKMhrKL377rvq1auXmjZtqlatWmn06NE6duyYe7xjx47ua4KtW7f2uEb41ltvqWvXroqIiFDbtm31zDPPKCMjw7aN+Ph4devWTd9995369++vZs2a6aWXXpIk5eTkaPbs2brzzjsVERGh22+/XS+88ILH99SbuXPnKigoSFOmTPE4Om3atKkSEhK0d+9ebdiwwfZ8xo8f77Gugt+31NRU9enTR5KUmJjoPv2flJRUaC3eriGnp6crMTFRt912myIiItS1a1etWbPGtkz+62P9+vV6+eWX1a5dOzVr1kxZWVkXff7FtWfPHo0fP16xsbGKjIxUmzZtlJiY6HFKf86cOQoLC9OhQ4c0fvx4tWjRQrfccosSExN19uxZ27I5OTmaNm2abr31VkVHR2vYsGH6+eefi1XPH91HirPfwTc4QjZAenq6+vTpo8zMTPXr10/169dXenq6NmzYoHPnzikgIEBHjhzRxo0b1aVLF9WqVUu//vqr3n77bQ0YMEDr169XaGiobZ2vv/66KleurKFDh+rQoUNavny5KlasKIfDoYyMDI0YMULffvutkpKSVLNmTY0YMcL2+K+++kopKSmKj49XQECAVq5cqYSEBK1evVpOp1OStGPHDm3dulVdu3ZVjRo1lJaWppUrV2rgwIFav359kUdwK1as0LPPPqsWLVrooYceUlpamoYPH67AwEDVqFHDY/mFCxfK4XBo8ODBysrK0qJFizR27FitXr26FL4DdqdPn9bx48clSadOndL777+vvXv36rnnnrMtN3/+fM2aNUtxcXHq06ePjh8/ruXLl6t///5at26dAgMDNWHCBK1bt04ff/yxJk+erCpVqigsLEzS7z/A586dq9tuu00PPPCADhw4oJUrV2rHjh1auXKl7RT/yZMn9fDDD6tr167q0aOHrr/+erlcLj366KP65ptv1K9fPzVo0EB79+7V0qVLdfDgQb366quFPseDBw/qwIED6tWrl6pVq+Z1mXvuuUdz5szR5s2b1bVr12LPX4MGDTRq1CjNnj1b9913n2655RZJUvPmzYu9jl9//VX9+vWTw+FQ//79FRISon/961+aOHGisrKy9NBDD9mWf/XVV+Xv768hQ4YoJyfnopdHsrOz3d/jgs6cOePR9/nnn+vIkSPq1auXqlevrh9++EGrVq3Sjz/+qFWrVsnhcNiWf/zxx1WrVi2NGTNGu3bt0urVqxUSEqInnnjCvczEiRP13nvvqVu3bmrevLm2bNmioUOHFnt+vCnOPnKp+x0uMws+9+STT1rh4eHW9u3bPcZcLpdlWZaVnZ1t5eXl2caOHDliRUREWHPnznX3bdmyxXI6nVa3bt2snJwcd/+YMWOssLAwKyEhwbaO++67z+rQoYOtz+l0Wk6n09qxY4e7Ly0tzYqMjLSGDx/u7jt79qxHvVu3brWcTqf1zjvveNS0ZcsW93Np1aqV1bt3bys3N9e9XFJSkuV0Oq0BAwZ4PDYuLs7Kzs529y9dutRyOp3W999/71FDSeVv68Kv8PBwa/78+bZljx49ajVq1Mij//vvv7caN25s6589e7bldDqt3377zd3322+/WU2aNLEGDx5s+74uX77ccjqd1po1a9x9AwYMsJxOp7Vy5UrbttatW2eFh4dbX331la1/5cqVltPptL755ptCn+vHH39sOZ1Oa8mSJUXOSfPmza17773X3e7QoYM1btw4j+UGDBhg+75t377dcjqd1tq1az2WHTdunNfX3OzZs93tCRMmWG3atLGOHz9uW2706NHWLbfc4n7t5X/PYmNjvb4evfH2Pb7wq+C+6G2977//vuV0Om1zn/99TkxMtC07fPhwq1WrVu727t27LafTaU2ePNm23JgxYzzmYe3atZbT6bSOHDni7rtwrou7j1zKfgff4JS1j7lcLm3cuFEdOnRw30hUUP6774CAAPc1sby8PJ04cUJVqlRRvXr1tGvXLo/H9ezZ03aU0LRpU1mWpd69e9uWa9q0qY4dO6bz58/b+qOjoxUREeFu33TTTYqNjdV//vMf5eXlSZIqV67sHs/NzdWJEydUp04dBQYGeq0p33fffaeTJ0+qX79+ttPA3bt3V1BQkNfH9OrVy3btLP+jQ0eOHCl0OyU1fPhwLVmyREuWLNHLL7+srl276uWXX9bSpUvdy3z88cdyuVyKi4vT8ePH3V833HCDbr755ot+xOvzzz9Xbm6uBg4caLvW2bdvX1WrVk2ffvqpbfmAgAD16tXL1vfhhx+qQYMGql+/vq2GW2+9VZKKrOH06dOSpKpVqxZZZ9WqVUv19G9xWJaljz76SB07dpRlWbbn1rZtW2VmZmrnzp22x9xzzz221+PFxMbGur/HBb+GDBnisWzB9eYfWTdr1kySPOqQpPvvv9/WbtGihU6ePOmex/zv7YWniAcNGlTs+r252D5Skv0OlxenrH3s+PHjysrK0p/+9Kcil3O5XHrzzTe1YsUKHT161B2KkhQcHOyx/E033WRr599QdOONN3r0u1wuZWZm6rrrrnP3e7sbuG7dujp79qyOHz+u6tWr69y5c1qwYIGSkpKUnp4uy7Lcy2ZmZhb6XH766SdJUp06dWz9FStWVM2aNb0+5sLnExgYKEke11sLysnJ0alTp2x9ISEhF72b1+l06rbbbnO37777bmVlZWnmzJnq3r27QkJCdPDgQVmWpbvuusvrOi52E1T+HFx4B3NAQIBq166ttLQ0W39oaKjHzTyHDh3Svn371Lp1a6/byL+ZzJv8IM4P5sKcPn1a119/fZHLlLbjx48rIyNDb7/9tt5+++1ClymoVq1al7SNGjVq2L7H+bxdxz158qTmzp2rlJQUjzn19jov7LV66tQpVatWTWlpafLz8/N4/ZfkbvbibDd/HynJfofLi0C+Qrz22muaNWuWevfurccee0xBQUHy8/PTtGnTbEGYr7A7TAvr97aOi5kyZYqSkpI0aNAgRUVF6dprr5XD4dDo0aNLtL6ilKTurVu3auDAgba+TZs2XfIPb0m69dZbtXnzZm3fvl133HGHXC6XHA6HFi5c6DXgq1SpcsnbKIq3oz+XyyWn06nExESvjynqmmCDBg0kSd9//32hy6SlpSkrK8u9bFHy8vJK7WNLLpdLktSjRw/de++9XpfJvw6f71KOji/V448/rq1bt2rIkCFq1KiRqlSpIpfLpYSEhEva90p7nzBluyg9BLKPhYSEqFq1avrhhx+KXG7Dhg2KiYnRtGnTbP0ZGRm2I9vS4u3uzIMHD+qaa65RSEiIu6Z77rnHdtdtdnZ2kUfH0n/fyR8+fNh9elWSzp8/r7S0NI8ftiUVHh6uJUuW2PqqV69eonXln5HIv+mnTp06sixLtWrVUr169S55fflzsH//ftWuXdvdn5OTo6NHj3o9ertQnTp1tGfPHrVu3drjxqKLqVevnurWratNmzYpKyvL641d69atkyR16NDB3RcUFOT1rMRPP/1kex6XWk9BISEhqlq1qlwuV7HmoSydOnVKX3zxhUaOHGm78fHgwYMlXmfNmjXlcrl0+PBh21Hx/v37/0ipF3W59juUHNeQfczPz0+dOnXS5s2bvf52oPx3txUqVPB4p/vBBx8oPT29TOraunWr7frYsWPHtGnTJrVp08Z9JOTtiGjZsmW20+neREREKDg4WKtWrbJdu05OTvY4xfxHBAUF6bbbbrN9lfTzrv/85z8l/ffI7K677lKFChU0d+5cj++LZVkX/S1Xt912m/z9/bVs2TLb49esWaPMzEzdfvvtF60pLi5O6enpWrVqlcfYuXPnvN4xXNDw4cN16tQpPf300x7fs++++06LFi2S0+m0nZavXbu2vv32W9vHqjZv3mz7qJck9x32RV1SKEyFChXUuXNnbdiwQXv37vUY93Z3dFkp7Ki/4P0El6p9+/aSft9XSmudxXG59juUHEfIBhgzZow+++wzxcfHuz++8ssvv+jDDz/UihUrFBgYqDvuuEPz5s1TYmKioqOjtXfvXiUnJ9uOSkqT0+nUkCFDbB97kqSRI0e6l7njjjv07rvvqlq1amrYsKG2bdumzz//3Os17YICAgI0cuRITZkyRYMGDVJcXJzS0tKUlJTkcX3LF77++mtlZ2dL+v0I6ZNPPtGXX36prl27uk/f1qlTR48//rhmzpyptLQ0derUSVWrVtXRo0e1ceNG9evXz+sNQvlCQkL0yCOPaO7cuUpISFDHjh114MABrVixQpGRkerRo8dF6+zZs6c++OADPf3000pNTVXz5s2Vl5en/fv368MPP9SiRYu83iiYr0ePHtqxY4fefPNN7du3T927d3ffkLd27VoFBwdr1qxZtpsD+/btqw0bNighIUFxcXE6fPiwkpOTPb5v+Tf3/eMf/1DVqlVVpUoVNW3atNiv1//5n/9Ramqq+vXrp759+6phw4Y6deqUdu7cqS+++EJffvllsdbzR1WrVk0tW7bUokWLlJubq9DQUH322Wcen7W/FI0aNVK3bt20YsUKZWZmKjo6Wlu2bCnzz9Wbvt+BQDZCaGioVq1apVmzZik5OVlZWVkKDQ1V+/bt3dfGhg0bprNnzyo5OVkpKSlq3LixFixYoJkzZ5ZJTS1btlRUVJTmzZunn376SQ0bNtT06dMVHh7uXmbixIny8/NTcnKysrOz1bx5cy1ZskQJCQkXXf+AAQNkWZaWLFmiGTNmKDw8XPPnz9fUqVN9/lu9Ch65+Pv7q3bt2ho9erRHwA4dOlR169bVG2+8oXnz5kn6/bptmzZtPP54gjcjR45USEiIli9frunTpysoKEj9+vXTmDFjivVrRv38/DRv3jy98cYbevfdd/Xxxx/rmmuuUa1atRQfH1+sU+kTJ05UTEyMVqxYoQULFujs2bO68cYb1b9/fz388MPuyxP52rVrp/Hjx2vJkiWaNm2aIiIi9Nprr2nGjBm25fz9/fX888/rpZde0uTJk3X+/HlNnz692IF8ww03aPXq1Zo3b54+/vhjrVy5UsHBwWrYsKHGjh1brHWUlpkzZ2rKlClasWKFLMtSmzZttHDhQrVr167E65w2bZquu+46JScna9OmTYqJidHrr79erDMjf4TJ+x0kh8UVf1wgLCxM/fv316RJky7rdl0ul1q3bq0777xTU6dOvazbBsor9jtzcA0ZPpGdne1x7XXdunU6efKkWrVq5aOqgKsb+53ZOGUNn9i2bZumT5+uLl26KDg4WLt27dKaNWvkdDptf9gBQOlhvzMbgQyfqFmzpmrUqKFly5bp1KlTCgoKUs+ePTV27Fj+mg1QRtjvzMY1ZAAADMA1ZAAADEAgAwBggGJdQ966dassyyrWZyMBAMB/5ebmyuFwKDo6usjlinWEbFlWiX5BuWVZysnJ4ZebF8Cc2DEfdsyHHfNhx3zYXSnzUdwMLdYRcv6RcVG/hs+bM2fOaPfu3WrYsGGp//WbKxVzYsd82DEfdsyHHfNhd6XMh7e/U+AN15ABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADFDRFxu1LEs5OTlFjkuSw+EodJmAgIAixwEAuJL4JJBzcnI0bty4P7SOGTNmqFKlSqVUEQAAvsUpawAADOCTI+SC6vV6WH4V/d1t1/lcHUhaeNExAACuJj4PZL+K/rbQLe4YAABXE05ZAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYIByH8iWZcmyLF+XAQAo53wSyAUD0JdhaFmWZs+erdmzZxPKAACfquiLjebm5rr/b+Wdl/wDfFGGcnJydODAAff/K1Wq5JM6AAAo96esAQAwAYEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAAFX1dgMkef/xx9/9feeWVUhmbMGGC17Gy2FZJxi53HYXNh0k1Xs46SjIfl7vG0q5j3Lhxys7OVqVKlTRjxgzbWEnn44033tC2bdsUFRWlhx56qFiPGzNmjFwul/z8/PTSSy/ZHjNr1iwdOHBA9erV02OPPVbs+ouyfv16bdq0SbGxseratWuxHjNx4kRZliWHw6GXX37ZY+z06dOqWrWqnnvuuWLXURLfffed1q5dq969eysiIsLYdV5pdXCEXIiCO+2F7dIeu5zbuhLquBJqNKWOktb417/+1TZWsP3EE0/Yxgq2Z82aZRsr2J4zZ45tLL9dWL8k7dy5U9nZ2ZKk7Oxs7dy50z326aef2h5XsL127VrbWMH28ePHtW3bNknStm3bdPz4cfdYSkqK7XH57W+++UYul0uS5HK59M0337iXSU9P14EDByRJBw4cUHp6erHqL0pWVpY2btwol8uljRs3Kisr66KPSU1NlWVZkiTLspSamuoe27t3r06fPi1JOn36tPbu3VusOkoiJydHq1ev1okTJ7R69Wrl5OQYuc4rsQ4CGSiHLgyAgu3c3FzbWMF2fjB5a+/bt882lt8urF+SFi5caBsr2H7nnXdsYwXb//73v21jBduzZ8+2jRVsf/TRR7ax/PayZcts/QXbFx4tF2wXVX9RFi9ebAvXxYsXX/QxK1euLLT96quv2sYubJemjRs3KiMjQ5KUkZGhjRs3GrnOK7EOh5X/qijCjh07JEmRkZGXtPIzZ85o9+7datSokapUqeLuz8zM1FNPPSVJqtfrYVWs/N8x1/lc7Vv1+4upQb+/yK+iv9exKVOmKCAg4JLquVBOTo67jhkzZqhSpUqSPI8sAJS+Ro0aaffu3V7709PTbUe2+UJCQpSZmenxpkGS/P391bdvX61YscJj7MEHH9S6det05syZYtdXp04dNWvWTMnJyR5j3bt3148//lho/Y888kih6/3+++81f/58j/5HH31UYWFhXh8zY8YMHTt2zKP/xhtvVGhoqPuMQEHeTtf/Ub/88oumT5/uPpsgSRUqVND48eNVvXr1y77OwjLmctdxMcXN0CvqGnLB9w75QVra6yaMgcvDW5gV1S/Ja0jny83N9RrGkgrtL8rhw4d1+PBhr2PeQjrf7t27de7cOVWuXNljzOVyaenSpV4ft3TpUk2dOlV+fvYTl6dPn/YaxpJ07NixQse2bdvmvrZdGizL8rhUULD/kUcekcPh8Pk6S8KUOjhlDQCl7LXXXvPav2vXrkKP0s+cOaNdu3Z59L/44oslrqM4p8KLKz09XXv27LEdQUq/v8nYs2eP7dq6L9dZEqbUcUUdIRd8h1Lap6wvx7sfAOXDsGHDvPY3btxYVapU8RrKVatWVePGjT36x44dq4kTJ5aojsGDB5focd6EhoYqPDxce/futQWXn5+fnE6nQkNDjVhnSZhSxxV7hBwQEKBKlSr9oS9vgX7hxycAlI1GjRp57W/cuLFCQkK8joWEhMjf39/rmL+/vx588EGvY/3797/ka4w333yzunfv7nWsR48eRdbv7XS19PsP+EGDBnkdGzRokMfpaun3oL7xxhu9Puamm25SVFSU17Ho6OhSO10t/X7Q0rt3b6/9ffr0KdFBTVmssyRMqeOKDeSyVFgov/LKK2Uydjm3dSXUcSXUaEodV0KNhfUXduPT0KFDNWnSJK9jkyZN0t/+9jevY3/729/UqlUrBQcH2/qDg4PVsmVLTZs27ZJqHD16tGJjYz1CrVKlSurYsWOR9RclLCxM9evXt/XVr19fTqez0MeMGzfOa/+TTz5Z6I1bhQX/H1G9enV16tTJHVAOh0OxsbG64YYbjFrnlVoHgQyUQ9WqVSu0feERaMF2vXr1bGMF2w0aNLCN5bcL65ekhx9+2DZWsH3vvffaxgq227VrZxsr2B41apRtrGD7rrvuso3lt+Pj4239BdtjxoyxjRVsF1V/UQYPHmz7wV+cU8sPPPBAoe2//OUvtrEL26WpU6dOCgwMlCQFBQWpU6dORq7zSqyDQC7Ehe+aC7ZLe+xybutKqONKqNGUOkpa49SpU21jBdsXHoEWbF/4W6oKtkeOHGkby28X1i9JTZo0cR+BVqpUSU2aNHGP3X777bbHFWxfeHqxYDskJMR9GjcqKsp2+vvuu++2PS6/fcstt7hPF/v5+emWW25xLxMaGup+41GvXj3b9cSi6i9KtWrV1KlTJ/n5+alTp04eb5C8iYmJsYV4TEyMe8zpdKpq1aqSfj/FXdTR9h8VEBCgvn376rrrrlOfPn3+8L08ZbXOK7GOK/ZzyAU/N1xS2dnZ7lNBpbG+4ijNz81dDZgPO+bDjvmwYz7srpT5KG6GcoQMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAao6IuN+vv7u//vqOCTEiRJAQEBqlevnvv/AAD4ik/S0OFweP2/L+oYNWqUz+sAAMB3h6eGIIgBACbgGjIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAABV9XYDrfG6h7aLGAAC4mvg8kA8kLSzRGAAAVxNOWQMAYACfHCEHBARoxowZhY5bliVJcjgcRa4DAICrhU8C2eFwqFKlSr7YNAAARuKUNQAABiCQAQAwAIEMAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiCQAQAwAIEMAIABHJZlWRdb6H//939lWZYCAgIuaeWWZSk3N1f+/v5yOBwlLvJqwpzYMR92zIcd82HHfNhdKfORk5Mjh8Oh5s2bF7lcxeKsrKRP1OFwXHKIX+2YEzvmw475sGM+7JgPuytlPhwOR7FytFhHyAAAoGxxDRkAAAMQyAAAGIBABgDAAAQyAAAGIJABADAAgQwAgAEIZAAADEAgAwBgAAIZAAADEMgAABiAQAYAwAAEMgAABiizQN63b5/+/Oc/KyoqSm3atNELL7ygnJycstqcUQ4dOqRJkyapZ8+eaty4sbp16+Z1udWrV6tz586KjIxUjx49tHnz5stcadn74IMP9Oijj6p9+/aKiopSz549tWbNGl34N03Kw1xI0qeffqoBAwbo1ltvVUREhGJjYzV9+nRlZmbalvvkk0/Uo0cPRUZGqnPnzlq7dq2PKr68Tp8+rfbt2yssLEw7duywjZWX10hSUpLCwsI8vl588UXbcuVlPvK98847uueeexQZGamYmBglJCTo3Llz7vGrYZ8p1p9fvFSnTp3SoEGDVLduXc2ZM0fp6el6/vnnde7cOU2aNKksNmmUH374QZ9++qmaNWsml8vlET6StH79ej311FMaNmyYbr31VqWkpGjEiBF66623FBUVdfmLLiNvvPGGatasqfHjx+u6667T559/rqeeeko///yzRowYIan8zIUknTx5Uk2bNlV8fLyCg4P1ww8/aM6cOfrhhx+0ePFiSdLXX3+tESNGqE+fPpowYYK2bNmiiRMnqmrVqurSpYuPn0HZevXVV5WXl+fRX55eI/kWLVqka6+91t0ODQ11/7+8zcf8+fO1cOFCDRs2TFFRUTpx4oS++OIL92vlqtlnrDLw2muvWVFRUdaJEyfcff/4xz+sRo0aWT///HNZbNIoeXl57v+PGzfO6tq1q8cyd911lzVmzBhb33333WclJCSUeX2X02+//ebR99e//tVq3ry5e57Ky1wU5u2337acTqd73xg8eLB133332ZYZM2aMFRcX54vyLpsff/zRioqKslauXGk5nU5r+/bt7rHy9BpZu3at5XQ6ve47+crTfOzbt89q3Lix9c9//rPQZa6WfaZMTln/61//UuvWrRUcHOzui4uLk8vl0meffVYWmzSKn1/R03rkyBEdPHhQcXFxtv67775bX3zxxVV1aj8kJMSjr1GjRsrKytKZM2fK1VwUJn8/yc3NVU5OjlJTUz3e1d99993at2+fjh496oMKL4+pU6fq/vvvV7169Wz9vEbsytt8JCUlqVatWrr99tu9jl9N+0yZBPL+/ftVv359W19gYKCqV6+u/fv3l8Umryj5c3DhD54GDRooNzdXR44c8UVZl80333yj0NBQVatWrdzORV5enrKzs7Vz507NmzdPHTt2VK1atXT48GHl5uZ67D8NGjSQpKt2//nwww+1d+9eDR8+3GOsvL5GunXrpkaNGik2NlYLFixwn54tb/Px7bffyul06tVXX1Xr1q0VERGh+++/X99++60kXVX7TJlcQ87IyFBgYKBHf1BQkE6dOlUWm7yi5M/BhXOU376a5+jrr79WSkqKxo0bJ6n8zkWHDh2Unp4uSWrXrp1mzpwpqXzOx9mzZ/X8889r9OjRqlatmsd4eZuT6tWra+TIkWrWrJkcDoc++eQTvfLKK0pPT9ekSZPK3Xz88ssv+u6777R37149/fTTuuaaa/Taa69p8ODB+uijj66q+SiTQAa8+fnnnzV69GjFxMRo4MCBvi7Hp15//XWdPXtWP/74o+bPn69hw4ZpyZIlvi7LJ+bPn6/rr79evXv39nUpRmjXrp3atWvnbrdt21aVKlXS0qVLNWzYMB9W5huWZenMmTOaNWuWwsPDJUnNmjVTx44dtXz5crVt29bHFZaeMjllHRgY6PExDun3dypBQUFlsckrSv4cXDhHGRkZtvGrSUZGhh5++GEFBwdrzpw57uvs5XEuJCk8PFzR0dHq27evXn31VaWmpurjjz8ud/ORlpamxYsXa9SoUcrMzFRGRobOnDkjSTpz5oxOnz5d7ubEm7i4OOXl5Wn37t3lbj4CAwMVHBzsDmPp9/suGjdurB9//PGqmo8yCeT69et7nLfPzMzUL7/84nGevzzKn4ML52j//v3y9/dX7dq1fVFWmTl37pweeeQRZWZmenyUo7zNhTdhYWHy9/fX4cOHVadOHfn7+3udD0lX3f5z9OhR5ebmaujQoWrZsqVatmzpPgocOHCg/vznP/MauUB5m4+GDRsWOpadnX1V7TNlEsjt27fX559/7n6HIv1+04afn5/atGlTFpu8otSuXVt169bVhx9+aOtPSUlR69atFRAQ4KPKSt/58+f1+OOPa//+/Vq0aJHts5RS+ZqLwnz77bfKzc1VrVq1FBAQoJiYGG3YsMG2TEpKiho0aKBatWr5qMqy0ahRI7355pu2r8TEREnSM888o6effprXiH5/rhUqVFDjxo3L3Xx06NBBJ0+e1O7du919J06c0M6dO9WkSZOrap8pk2vI999/v5YtW6bhw4frkUceUXp6ul544QXdf//9Hj+Qr0Znz57Vp59+Kun3U3JZWVnunadVq1YKCQnRyJEjNXbsWNWpU0cxMTFKSUnR9u3btXz5cl+WXuqeeeYZbd68WePHj1dWVpa2bdvmHmvcuLECAgLKzVxI0ogRIxQREaGwsDBVrlxZe/bs0d///neFhYWpU6dOkqRHH31UAwcO1OTJkxUXF6fU1FS9//77evnll31cfekLDAxUTEyM17EmTZqoSZMmklSuXiNDhgxRTEyMwsLCJEmbNm3SqlWrNHDgQFWvXl1S+ZqPTp06KTIyUqNGjdLo0aNVqVIlvf766woICNCDDz4o6erZZxyW5eXXSJWCffv2acqUKdq6dauqVq2qnj17avTo0Vfduzdvjh49qtjYWK9jb775pvsH0OrVq7Vw4UL99NNPqlevnsaMGaMOHTpczlLLXMeOHZWWluZ1bNOmTe53r+VhLqTfb+ZKSUnR4cOHZVmWatasqTvvvFNDhgyx3WG8adMmvfLKKzpw4IBuuukmDR06VH369PFh5ZdPamqqBg4cqDVr1igyMtLdX15eI1OnTtW///1v/fzzz3K5XKpbt6769u2r+Ph4ORwO93LlZT4k6fjx45o+fbo2b96s3NxctWjRQomJibbT2VfDPlNmgQwAAIqPv/YEAIABCGQAAAxAIAMAYAACGQAAAxDIAAAYgEAGAMAABDIAAAYgkAEoNTVVYWFhSk1N9XUpQLlFIAMAYAB+UxcAuVwu5ebmyt/f3/2nMQFcXgQyAAAG4K0wcAnS09M1YcIEtW3bVhEREerYsaOefvpp5eTk6OTJk5oxY4a6d++u6OhoNW/eXAkJCdqzZ49tHfnXa1NSUjR37ly1a9dO0dHRGjVqlDIzM5WTk6PnnntOrVu3VnR0tBITE5WTk2NbR1hYmJ599lm999576ty5syIjI9WrVy999dVXtuXS0tI0efJkde7cWU2bNlVMTIxGjRqlo0ePeq3pwmvIb731lmJjY9W0aVP16dNHX3/9teLj4xUfH+/1+cyfP1/t27dXZGSkBg0apEOHDpXGtAPlQpn8+UXgapSenq4+ffooMzNT/fr1U/369ZWenq4NGzbo3LlzOnLkiDZu3KguXbqoVq1a+vXXX/X2229rwIABWr9+vcefHn399ddVuXJlDR06VIcOHdLy5ctVsWJFORwOZWRkaMSIEfr222+VlJSkmjVrasSIEbbHf/XVV0pJSVF8fLwCAgK0cuVKJSQkaPXq1XI6nZKkHTt2aOvWreratatq1KihtLQ0rVy5UgMHDtT69et1zTXXFPp8V6xYoWeffVYtWrTQQw89pLS0NA0fPlyBgYGqUaOGx/ILFy6Uw+HQ4MGDlZWVpUWLFmns2LFavXp1Kcw+UA5YAIrlySeftMLDw63t27d7jLlcLis7O9vKy8uz9R85csSKiIiw5s6d6+7bsmWL5XQ6rW7dulk5OTnu/jFjxlhhYWFWQkKCbR333Xef1aFDB1uf0+m0nE6ntWPHDndfWlqaFRkZaQ0fPtzdd/bsWY9at27dajmdTuudd97xqGnLli2WZVlWdna21apVK6t3795Wbm6ue7mkpCTL6XRaAwYM8HhsXFyclZ2d7e5funSp5XQ6re+//96jBgCeOGUNFIPL5dLGjRvVoUMH29/ozedwOBQQEOC+ISovL08nTpxQlSpVVK9ePe3atcvjMT179pS/v7+73bRpU1mWpd69e9uWa9q0qY4dO6bz58/b+qOjoxUREeFu33TTTYqNjdV//vMf5eXlSZIqV67sHs/NzdWJEydUp04dBQYGeq0p33fffaeTJ0+qX79+qljxvyfSunfvrqCgIK+P6dWrl+3vnbdo0UKSdOTIkUK3A+C/OGUNFMPx48eVlZWlP/3pT4Uu43K59Oabb2rFihU6evSoOxQlKTg42GP5m266yda+9tprJUk33nijR7/L5VJmZqauu+46d//NN9/ssc66devq7NmzOn78uKpXr65z585pwYIFSkpKUnp6uqwC93BmZmYW+lx++uknSVKdOnVs/RUrVlTNmjW9PubC5xMYGChJysjIKHQ7AP6LQAZKyWuvvaZZs2apd+/eeuyxxxQUFCQ/Pz9NmzbNFoT5Cvt4UWH93tZxMVOmTFFSUpIGDRqkqKgoXXvttXI4HBo9enSJ1leU0qwbKI8IZKAYQkJCVK1aNf3www+FLrNhwwbFxMRo2rRptv6MjAzbkW1p8XYH88GDB3XNNdcoJCTEXdM999yj8ePHu5fJzs4u8uhY+u/R7uHDh3Xrrbe6+8+fP6+0tDSFhYWVxlMAUADXkIFi8PPzU6dOnbR582bt2LHDY9yyLFWoUMHjaPCDDz5Qenp6mdS0detW7dy5090+duyYNm3apDZt2qhChQqS5P63oGXLltlOp3sTERGh4OBgrVq1ynbtOjk5WadOnSqlZwCgII6QgWIaM2aMPvvsM8XHx6tfv35q0KCBfvnlF3344YdasWKF7rjjDs2bN0+JiYmKjo7W3r17lZycrNq1a5dJPU6nU0OGDLF97EmSRo4c6V7mjjvu0Lvvvqtq1aqpYcOG2rZtmz7//HOv17QLCggI0MiRIzVlyhQNGjRIcXFxSktLU1JSksd1ZQClg0AGiik0NFSrVq3SrFmzlJycrKysLIWGhqp9+/aqXLmyhg0bprNnzyo5OVkpKSlq3LixFixYoJkzZ5ZJPS1btlRUVJTmzZunn376SQ0bNtT06dMVHh7uXmbixIny8/NTcnKysrOz1bx5cy1ZskQJCQkXXf+AAQNkWZaWLFmiGTNmKDw8XPPnz9fUqVNVqVKlMnlOQHnGr84ErkBhYWHq37+/Jk2adFm363K51Lp1a915552aOnXqZd02cLXjGjIAr7Kzsz2uia9bt04nT55Uq1atfFQVcPXilDUAr7Zt26bp06erS5cuCg4O1q5du7RmzRo5nU516dLF1+UBVx0CGYBXNWvWVI0aNbRs2TKdOnVKQUFB6tmzp8aOHWv7jVwASgfXkAEAMADXkAEAMACBDACAAQhkAAAMQCADAGAAAhkAAAMQyAAAGIBABgDAAAQyAAAGIJABADDA/wMTduAFcRzPMAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# === Encode categorical features ===\ncategorical_cols = [\"job\", \"marital\", \"education\", \"contact\", \"month\", \"poutcome\"]\n\n# Create a label encoder for each categorical column\nlabel_encoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    label_encoders[col] = le  # save encoder for possible inverse transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:04.115537Z","iopub.execute_input":"2025-08-12T10:29:04.115832Z","iopub.status.idle":"2025-08-12T10:29:04.816358Z","shell.execute_reply.started":"2025-08-12T10:29:04.115807Z","shell.execute_reply":"2025-08-12T10:29:04.815833Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === Define features and target ===\ntarget_col = 'y'  # y is the target column\nX = train.drop(columns=[target_col, 'id'])  # Drop target + id\ny = train[target_col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:04.818461Z","iopub.execute_input":"2025-08-12T10:29:04.818692Z","iopub.status.idle":"2025-08-12T10:29:04.872016Z","shell.execute_reply.started":"2025-08-12T10:29:04.818670Z","shell.execute_reply":"2025-08-12T10:29:04.871403Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom lightgbm import LGBMClassifier\nimport numpy as np\n\n# === Train-Test Split ===\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    stratify=y,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:04.872685Z","iopub.execute_input":"2025-08-12T10:29:04.872864Z","iopub.status.idle":"2025-08-12T10:29:05.281317Z","shell.execute_reply.started":"2025-08-12T10:29:04.872851Z","shell.execute_reply":"2025-08-12T10:29:05.280753Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# === Define LightGBM model ===\nlgb_model = LGBMClassifier(\n    boosting_type='gbdt',\n    n_estimators=1000,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:29:05.282018Z","iopub.execute_input":"2025-08-12T10:29:05.282209Z","iopub.status.idle":"2025-08-12T10:29:05.286534Z","shell.execute_reply.started":"2025-08-12T10:29:05.282194Z","shell.execute_reply":"2025-08-12T10:29:05.285775Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# === Fit model ===\nlgb_model.fit(X_train, y_train)\n\n# === Predictions ===\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\n\n# === Accuracy ===\ntrain_acc = accuracy_score(y_train, y_train_pred)\ntest_acc = accuracy_score(y_test, y_test_pred)\n\nprint(f\"Train Accuracy: {train_acc:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# === Additional Metrics ===\nprint(\"\\nClassification Report (Test):\")\nprint(classification_report(y_test, y_test_pred))\n\nprint(\"\\nConfusion Matrix (Test):\")\nprint(confusion_matrix(y_test, y_test_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:30:50.790893Z","iopub.execute_input":"2025-08-12T10:30:50.791172Z","iopub.status.idle":"2025-08-12T10:31:31.275595Z","shell.execute_reply.started":"2025-08-12T10:30:50.791154Z","shell.execute_reply":"2025-08-12T10:31:31.274785Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030246 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTrain Accuracy: 0.9389\nTest Accuracy: 0.9330\n\nClassification Report (Test):\n              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96    131902\n           1       0.75      0.66      0.70     18098\n\n    accuracy                           0.93    150000\n   macro avg       0.85      0.82      0.83    150000\nweighted avg       0.93      0.93      0.93    150000\n\n\nConfusion Matrix (Test):\n[[127965   3937]\n [  6112  11986]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Model definition\nlgb_model = LGBMClassifier(\n    boosting_type='gbdt',\n    n_estimators=1000,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=31,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Fit with callbacks for early stopping & logging\nlgb_model.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_test, y_test)],\n    eval_metric='accuracy',\n    callbacks=[\n        early_stopping(stopping_rounds=100),\n        log_evaluation(period=50)  # print metrics every 50 iterations\n    ]\n)\n\n# Predictions\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\n\n# Metrics\nprint(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\nprint(\"\\nClassification Report (Test):\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\\nConfusion Matrix (Test):\")\nprint(confusion_matrix(y_test, y_test_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:31:31.277030Z","iopub.execute_input":"2025-08-12T10:31:31.277273Z","iopub.status.idle":"2025-08-12T10:32:21.283942Z","shell.execute_reply.started":"2025-08-12T10:31:31.277256Z","shell.execute_reply":"2025-08-12T10:32:21.283105Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031054 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 100 rounds\n[50]\ttraining's binary_logloss: 0.180345\tvalid_1's binary_logloss: 0.180497\n[100]\ttraining's binary_logloss: 0.163623\tvalid_1's binary_logloss: 0.164335\n[150]\ttraining's binary_logloss: 0.157444\tvalid_1's binary_logloss: 0.158801\n[200]\ttraining's binary_logloss: 0.154144\tvalid_1's binary_logloss: 0.156138\n[250]\ttraining's binary_logloss: 0.152079\tvalid_1's binary_logloss: 0.154637\n[300]\ttraining's binary_logloss: 0.15062\tvalid_1's binary_logloss: 0.153822\n[350]\ttraining's binary_logloss: 0.149196\tvalid_1's binary_logloss: 0.153071\n[400]\ttraining's binary_logloss: 0.147981\tvalid_1's binary_logloss: 0.152505\n[450]\ttraining's binary_logloss: 0.146978\tvalid_1's binary_logloss: 0.152187\n[500]\ttraining's binary_logloss: 0.145945\tvalid_1's binary_logloss: 0.151774\n[550]\ttraining's binary_logloss: 0.145043\tvalid_1's binary_logloss: 0.151494\n[600]\ttraining's binary_logloss: 0.144125\tvalid_1's binary_logloss: 0.151167\n[650]\ttraining's binary_logloss: 0.143249\tvalid_1's binary_logloss: 0.150915\n[700]\ttraining's binary_logloss: 0.142494\tvalid_1's binary_logloss: 0.150774\n[750]\ttraining's binary_logloss: 0.141703\tvalid_1's binary_logloss: 0.150582\n[800]\ttraining's binary_logloss: 0.141038\tvalid_1's binary_logloss: 0.150517\n[850]\ttraining's binary_logloss: 0.140368\tvalid_1's binary_logloss: 0.150395\n[900]\ttraining's binary_logloss: 0.139646\tvalid_1's binary_logloss: 0.150263\n[950]\ttraining's binary_logloss: 0.138923\tvalid_1's binary_logloss: 0.150103\n[1000]\ttraining's binary_logloss: 0.138213\tvalid_1's binary_logloss: 0.149958\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's binary_logloss: 0.138213\tvalid_1's binary_logloss: 0.149958\nTrain Accuracy: 0.9389\nTest Accuracy: 0.9330\n\nClassification Report (Test):\n              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96    131902\n           1       0.75      0.66      0.70     18098\n\n    accuracy                           0.93    150000\n   macro avg       0.85      0.82      0.83    150000\nweighted avg       0.93      0.93      0.93    150000\n\n\nConfusion Matrix (Test):\n[[127965   3937]\n [  6112  11986]]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# # === Stratified K-Fold ===\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# # === Cross-validation ===\n# cv_scores = cross_val_score(\n#     lgb_model,\n#     X, y,\n#     cv=cv,\n#     scoring='accuracy',\n#     n_jobs=-1\n# )\n\n# print(f\"CV Scores: {cv_scores}\")\n# print(f\"Mean CV Accuracy: {np.mean(cv_scores):.4f}\")\n# print(f\"Std Dev: {np.std(cv_scores):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:32:21.284925Z","iopub.execute_input":"2025-08-12T10:32:21.285241Z","iopub.status.idle":"2025-08-12T10:32:21.289176Z","shell.execute_reply.started":"2025-08-12T10:32:21.285213Z","shell.execute_reply":"2025-08-12T10:32:21.288524Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\nimport numpy as np\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfold_results = []\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y), 1):\n    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = LGBMClassifier(\n        boosting_type='gbdt',\n        n_estimators=1000,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=31,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    model.fit(\n        X_train_fold, y_train_fold,\n        eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n        eval_metric='accuracy',\n        callbacks=[\n            early_stopping(stopping_rounds=50),\n            log_evaluation(period=0)  # set to >0 to see logs\n        ]\n    )\n    \n    y_val_pred = model.predict(X_val_fold)\n    acc = accuracy_score(y_val_fold, y_val_pred)\n    fold_results.append(acc)\n    \n    print(f\"Fold {fold} Accuracy: {acc:.4f}\")\n\nprint(f\"\\nMean CV Accuracy: {np.mean(fold_results):.4f}  {np.std(fold_results):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T10:32:21.290321Z","iopub.execute_input":"2025-08-12T10:32:21.290528Z","iopub.status.idle":"2025-08-12T10:35:10.101585Z","shell.execute_reply.started":"2025-08-12T10:32:21.290514Z","shell.execute_reply":"2025-08-12T10:35:10.100778Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030713 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 858\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's binary_logloss: 0.138701\tvalid_1's binary_logloss: 0.148623\nFold 1 Accuracy: 0.9342\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\ttraining's binary_logloss: 0.137812\tvalid_1's binary_logloss: 0.150523\nFold 2 Accuracy: 0.9328\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032839 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[999]\ttraining's binary_logloss: 0.137892\tvalid_1's binary_logloss: 0.150895\nFold 3 Accuracy: 0.9332\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029720 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 863\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's binary_logloss: 0.138408\tvalid_1's binary_logloss: 0.149104\nFold 4 Accuracy: 0.9338\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110120 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 862\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's binary_logloss: 0.138166\tvalid_1's binary_logloss: 0.149882\nFold 5 Accuracy: 0.9334\n\nMean CV Accuracy: 0.9335  0.0005\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Split data into train/test for final evaluation\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Base model\nlgb_base = LGBMClassifier(\n    boosting_type='gbdt',\n    objective='binary',\n    random_state=42,\n    n_jobs=-1\n)\n\n# Parameter grid\nparam_grid = {\n    'num_leaves': [31, 63, 127],\n    'max_depth': [-1, 8, 12],\n    'learning_rate': [0.03, 0.05, 0.1],\n    'n_estimators': [500, 1000],  # high so early stopping cuts it short\n    'min_child_samples': [20, 40],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\n# GridSearchCV setup\ngrid_search = GridSearchCV(\n    estimator=lgb_base,\n    param_grid=param_grid,\n    scoring='accuracy',\n    cv=3,                 # 3-fold CV\n    verbose=3,            # log progress for each fit\n    n_jobs=-1\n)\n\n# Fit with early stopping in each fold\ngrid_search.fit(\n    X_train_full, y_train_full,\n    **{\n        \"eval_metric\": \"accuracy\",\n        \"callbacks\": [\n            early_stopping(stopping_rounds=50, verbose=True),  # log early stopping info\n            log_evaluation(period=50)  # print every 50 iterations\n        ]\n    }\n)\n\n# Best results\nprint(\"\\nBest Parameters:\", grid_search.best_params_)\nprint(\"Best CV Accuracy:\", grid_search.best_score_)\n\n# Train final model with best params\nbest_lgb = grid_search.best_estimator_\nbest_lgb.fit(\n    X_train_full, y_train_full,\n    eval_set=[(X_test, y_test)],\n    eval_metric=\"accuracy\",\n    callbacks=[\n        early_stopping(stopping_rounds=50, verbose=True),\n        log_evaluation(period=50)\n    ]\n)\n\n# Evaluate on test set\ny_test_pred = best_lgb.predict(X_test)\nprint(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_test_pred))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_test_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T11:05:54.996878Z","iopub.execute_input":"2025-08-12T11:05:54.997369Z","iopub.status.idle":"2025-08-12T11:36:17.823747Z","shell.execute_reply.started":"2025-08-12T11:05:54.997346Z","shell.execute_reply":"2025-08-12T11:36:17.822787Z"}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228575 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.332340 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221534 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214649 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206896 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   1.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202816 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222985 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219872 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213449 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.358251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197793 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198267 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226602 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193580 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218787 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.331352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201880 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197877 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219462 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276983 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230786 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195317 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229234 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235766 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224797 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226345 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236409 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198893 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170416 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223618 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217355 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216369 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219796 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235053 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205868 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221495 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212301 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234452 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178457 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214574 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145002 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.194952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  12.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254951 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229826 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209083 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228136 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.257224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.5s\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241360 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223052 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202864 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226688 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275979 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204691 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208347 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205793 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306958 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238390 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235113 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227559 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238839 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214223 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187922 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212903 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236102 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.473666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223827 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175385 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.359973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225412 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149617 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203088 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228919 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261555 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202546 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200572 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276859 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217213 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221356 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129837 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217425 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187131 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191991 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223362 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204424 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179817 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185875 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292979 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226092 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266000 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238318 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194926 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.367836 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149793 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239927 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219651 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195427 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166448 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211370 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196473 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238851 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221206 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191884 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217350 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211519 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191933 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224862 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.271142 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212570 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.495966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177891 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217834 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176445 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184434 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226584 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202003 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090084 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225623 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255403 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.359756 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  12.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215774 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122126 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229806 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211278 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.353359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188031 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197616 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223880 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177288 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130141 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197855 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243614 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220803 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205836 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227027 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213399 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191153 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201712 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193517 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230606 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202531 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225691 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207387 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261001 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217371 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190886 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130070 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.283539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.258836 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205618 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202553 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206105 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208519 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200947 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154420 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197518 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207112 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255429 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215841 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305002 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217930 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235945 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249884 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194204 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243250 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128867 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   1.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227200 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190038 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213036 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224896 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.348003 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203403 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207059 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172414 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189358 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210346 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212381 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227280 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173453 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208600 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215913 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225447 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193105 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119818 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204535 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196242 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255510 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237047 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222289 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238654 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297989 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227863 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238396 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.270116 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125320 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   1.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212852 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202792 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178804 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.179197 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185355 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109800 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226451 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250398 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198242 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223840 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214822 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201801 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212061 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223133 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218814 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   7.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198164 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.190940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235428 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.0s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245043 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198900 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.244152 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232650 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.464977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195532 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225317 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201694 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186757 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151994 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175121 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255887 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144483 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235677 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241894 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214152 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223674 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.6s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213358 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243187 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190646 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202608 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255039 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259948 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211421 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178138 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193821 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287958 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229009 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221271 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.324556 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230240 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188289 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213099 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230854 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238956 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249845 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199099 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135218 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202663 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195026 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247987 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   7.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212644 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175887 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231347 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229492 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  12.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.278021 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238617 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144729 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232456 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.264837 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.283899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222608 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238427 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205915 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216954 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194300 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209027 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196257 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243019 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137152 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241344 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231872 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235464 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250405 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175659 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.262926 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160244 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189939 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165180 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101666 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   1.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206115 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  12.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   7.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180156 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216480 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216469 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134396 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212715 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202568 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183758 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204847 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.313947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.252816 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275610 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  12.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.311845 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233031 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202438 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203621 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175678 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238476 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.441255 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.259869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223659 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136872 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294473 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220800 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214071 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.244333 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181828 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213365 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187788 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258994 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245565 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215958 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229808 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202771 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.255803 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211591 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238573 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202831 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165751 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165198 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222827 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197881 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190806 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229632 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275978 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223628 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217447 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239643 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224996 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225918 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215885 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214968 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211163 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207544 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223399 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193284 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227518 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195758 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169834 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184932 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227299 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246631 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254959 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219151 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248292 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287994 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229891 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200418 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171824 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192448 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193017 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.142897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210663 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194720 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235892 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199412 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220477 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159104 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121886 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208388 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211946 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238287 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213144 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   7.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206194 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207156 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.168684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136645 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238817 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202444 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238405 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210272 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290967 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204401 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225940 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174485 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215828 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098926 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195894 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205387 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211281 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174297 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275928 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228330 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200890 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183298 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.390700 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   7.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216883 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199858 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224414 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226862 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190838 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.149670 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235997 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175551 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233709 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229154 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  12.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214890 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189431 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169574 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227892 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214869 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  12.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191474 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.281246 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248358 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204858 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195184 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153920 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   1.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239750 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.138524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202575 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221816 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162890 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217150 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238672 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189880 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197594 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202829 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219453 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232985 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229262 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213848 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346098 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212148 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182401 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  13.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194414 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184806 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214860 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203949 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.286425 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231715 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211929 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217830 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.303961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171933 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226963 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.419004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235760 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226383 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225353 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227564 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.267487 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.318986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.166896 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301978 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174874 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200144 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215498 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  13.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328992 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233759 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190057 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235918 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192867 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188424 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.394241 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219485 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251325 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   2.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231554 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248461 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187151 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243140 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230236 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200595 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215481 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225615 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217642 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227030 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184546 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233504 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123867 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214331 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239046 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.324009 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191877 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226019 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150773 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223716 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238483 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217892 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249455 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.154317 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203279 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238884 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226571 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216403 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213399 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  12.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341881 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170871 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205846 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238574 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216849 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229543 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.5s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169896 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234876 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213841 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   1.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232863 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182267 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253499 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218122 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165954 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210040 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190077 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223864 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198415 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210048 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209820 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223383 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200146 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239877 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235930 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224016 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.321964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198175 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180295 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212109 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177092 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234796 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236758 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212909 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250290 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187106 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226913 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174016 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192951 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241808 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254345 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.306539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203654 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108645 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236391 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239315 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227360 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261979 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.411466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204435 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.146878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207447 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204961 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215309 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264001 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.292910 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227340 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211721 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.188926 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184076 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.304698 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.145034 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173834 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202621 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287950 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228581 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190685 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150813 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225158 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.273540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202427 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162176 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198564 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.265735 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249369 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220397 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238662 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225891 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208864 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   1.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198512 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225432 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202412 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254415 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105813 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229397 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.130473 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163398 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212893 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239646 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207208 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178007 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199810 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194858 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.315178 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240153 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223493 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.311995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220843 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162781 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163776 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275176 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207247 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225952 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216796 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204088 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243145 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224537 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240409 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238428 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.271928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  12.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229606 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190407 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184894 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.03, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.244373 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228818 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229480 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228656 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217609 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211963 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213764 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.167964 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202915 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202461 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071814 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234842 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228114 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216910 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198612 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229866 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222368 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209659 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.172437 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.211294 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217296 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211530 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205881 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178496 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191772 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212616 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220432 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164551 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192439 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.116918 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201610 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.264430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225405 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229468 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225510 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.321839 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209147 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181849 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217606 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.393794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226336 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233979 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139649 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   1.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223139 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238909 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300978 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  14.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204772 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260720 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219646 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192364 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200477 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221460 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171935 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195571 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211184 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205852 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209704 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253378 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174668 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.364013 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225443 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215558 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215701 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226669 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203474 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234909 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251750 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229546 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.257234 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238362 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215556 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217366 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226562 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.212968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.3s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   1.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.331662 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203588 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.132121 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211955 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.194459 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219657 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202239 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196404 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187372 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175140 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213939 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210468 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214308 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   7.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249456 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248230 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243942 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213531 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237059 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221463 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223721 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224662 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212214 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   7.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.184717 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120841 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249664 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215802 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227337 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231376 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.143888 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209463 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248093 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182274 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148795 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224513 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.139625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.206974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193494 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213967 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233355 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249870 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   2.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214667 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224592 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235285 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241162 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229582 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238908 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224443 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.363833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.159651 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198426 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180426 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148817 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211179 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.193922 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221892 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.273793 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241767 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302950 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  12.4s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.584990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157252 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253172 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216248 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239443 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104016 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210964 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225429 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227274 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227662 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206854 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.260488 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222692 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195661 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222462 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210148 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247671 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245048 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323972 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250857 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.05, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.176136 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222612 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261979 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203958 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175819 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190664 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178401 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175207 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185460 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197586 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238475 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203255 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226747 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223270 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219603 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293953 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226948 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171955 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108849 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202160 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.253157 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202929 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208489 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   2.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231732 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.210472 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218229 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207773 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198389 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.163102 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   1.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.4s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243911 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.134478 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185389 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.191456 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   4.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190473 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.209631 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237995 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.362853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147002 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250561 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238809 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183908 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238202 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225962 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153838 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214932 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178732 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238322 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.165377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237170 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203196 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227301 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197887 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186853 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225179 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.157388 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241913 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222682 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   2.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206365 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.225970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   9.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220289 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.322131 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225368 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175969 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=-1, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.153376 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160826 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152506 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.206582 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205353 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216316 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239333 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238383 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205724 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237701 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.211201 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215460 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.266878 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203602 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223824 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.147668 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   7.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198135 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   1.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.215084 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.311977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  10.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237442 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243117 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283975 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233859 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225080 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.197193 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250664 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203170 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228567 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   2.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190495 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220827 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238346 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212658 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129088 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170454 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   7.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213313 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.212376 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228943 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   8.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238476 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308422 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230900 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204830 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226856 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=8, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.312373 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235206 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   2.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204266 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204920 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222946 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226944 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328106 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225846 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.310526 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   4.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261082 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217600 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.189899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=  10.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.196982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   4.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.141832 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   4.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216591 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   2.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250872 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220621 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219609 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   2.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230473 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308832 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254652 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   8.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230513 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   7.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.222080 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.196197 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.252921 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   6.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238612 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   9.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.226966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   4.3s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   6.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.181533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.190541 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=   6.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.220700 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160733 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.262902 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   3.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=   9.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.214941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.205521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.316796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.198844 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   8.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.201628 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   3.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238100 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3739170742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Fit with early stopping in each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m grid_search.fit(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mX_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     **{\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: \nAll the 1296 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1296 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\", line 955, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\", line 317, in train\n    cb(\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py\", line 399, in __call__\n    self._init(env)\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py\", line 324, in _init\n    raise ValueError(\"For early stopping, at least one dataset and eval metric is required for evaluation\")\nValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n"],"ename":"ValueError","evalue":"\nAll the 1296 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1296 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/sklearn.py\", line 955, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py\", line 317, in train\n    cb(\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py\", line 399, in __call__\n    self._init(env)\n  File \"/usr/local/lib/python3.11/dist-packages/lightgbm/callback.py\", line 324, in _init\n    raise ValueError(\"For early stopping, at least one dataset and eval metric is required for evaluation\")\nValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n","output_type":"error"},{"name":"stdout","text":"\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.9s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203782 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  10.8s\n\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   3.2s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120299 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   4.4s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.258597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238875 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   2.4s\n\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=0.8;, score=nan total time=  11.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180747 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   9.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207483 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.7s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.326801 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=  10.0s\n\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=20, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   6.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.170411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=31, subsample=1.0;, score=nan total time=   3.1s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161925 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=0.8;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169805 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=63, subsample=1.0;, score=nan total time=   6.5s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239364 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=500, num_leaves=127, subsample=1.0;, score=nan total time=   5.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=0.8;, score=nan total time=   4.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.217433 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=31, subsample=1.0;, score=nan total time=   3.8s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235360 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 864\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=63, subsample=1.0;, score=nan total time=   5.6s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263497 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=0.8;, score=nan total time=  11.0s\n[LightGBM] [Info] Number of positive: 48260, number of negative: 351740\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097856 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 860\n[LightGBM] [Info] Number of data points in the train set: 400000, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=12, min_child_samples=40, n_estimators=1000, num_leaves=127, subsample=1.0;, score=nan total time=   1.1s\n","output_type":"stream"}],"execution_count":22}]}